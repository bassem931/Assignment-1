{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "35roXDEMudbw"
      },
      "source": [
        "# GUC Clustering Project\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CIiItKbYudb2"
      },
      "source": [
        "**Objective:**\n",
        "The objective of this project teach students how to apply clustering to real data sets\n",
        "\n",
        "The projects aims to teach student:\n",
        "\n",
        "- Which clustering approach to use\n",
        "- Compare between Kmeans, Hierarchal, DBScan, and Gaussian Mixtures\n",
        "- How to tune the parameters of each data approach\n",
        "- What is the effect of different distance functions (optional)\n",
        "- How to evaluate clustering approachs\n",
        "- How to display the output\n",
        "- What is the effect of normalizing the data\n",
        "\n",
        "Students in this project will use ready-made functions from Sklearn, plotnine, numpy and pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtHElDYdudb3"
      },
      "outputs": [],
      "source": [
        "# if plotnine is not installed in Jupyter then use the following command to install it\n",
        "# use at anaconda prompt with it being run as admin\n",
        "# one time only then it is never needed again\n",
        "# conda install -c conda-forge plotnine\n",
        "# done\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5RHS5ZoQudb4"
      },
      "source": [
        "Running this project require the following imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrueqJenudb5"
      },
      "outputs": [],
      "source": [
        "from plotnine import *\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.datasets import make_blobs\n",
        "import sklearn.preprocessing as prep\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# StandardScaler is a function to normalize the data\n",
        "# You may also check MinMaxScaler and MaxAbsScaler\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# hierarchical clustering\n",
        "\n",
        "\n",
        "# for cdist in distortions graph\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju2Zj6-nudb5"
      },
      "outputs": [],
      "source": [
        "# helper function that allows us to display data in 2 dimensions an highlights the clusters\n",
        "def display_cluster(X, km=[], num_clusters=0):\n",
        "    alpha = 0.5  # color opaque\n",
        "    s = 20\n",
        "    if num_clusters == 0:\n",
        "        plt.scatter(X[:, 0], X[:, 1], c=\"r\", alpha=alpha, s=s)\n",
        "    else:\n",
        "        colors = list(map(lambda x: \"#3b4cc0\" if x == 1 else \"#b40426\", km.labels_))\n",
        "        for i in range(num_clusters):\n",
        "            plt.scatter(\n",
        "                X[km.labels_ == i, 0], X[km.labels_ == i, 1], c=colors, alpha=alpha, s=s\n",
        "            )\n",
        "            plt.scatter(\n",
        "                km.cluster_centers_[i][0],\n",
        "                km.cluster_centers_[i][1],\n",
        "                c=colors,\n",
        "                marker=\"o\",\n",
        "                s=100,\n",
        "            )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hZnIbT3Mudb6"
      },
      "source": [
        "## Multi Blob Data Set\n",
        "\n",
        "- The Data Set generated below has 6 cluster with varying number of users and varing densities\n",
        "- Cluster the data set below using\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeSqG318udb7",
        "outputId": "078fad92-3073-4558-b1e8-f0acd8d85d34"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = [8, 8]\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"talk\")\n",
        "\n",
        "n_bins = 6\n",
        "centers = [(-3, -3), (0, 0), (5, 2.5), (-1, 4), (4, 6), (9, 7)]\n",
        "Multi_blob_Data, y = make_blobs(\n",
        "    n_samples=[100, 150, 300, 400, 300, 200],\n",
        "    n_features=2,\n",
        "    cluster_std=[1.3, 0.6, 1.2, 1.7, 0.9, 1.7],\n",
        "    centers=centers,\n",
        "    shuffle=False,\n",
        "    random_state=42,\n",
        ")\n",
        "display_cluster(Multi_blob_Data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GDSIGjubudb8"
      },
      "source": [
        "### Kmeans\n",
        "\n",
        "- Use Kmeans with different values of K to cluster the above data\n",
        "- Display the outcome of each value of K\n",
        "- Plot distortion function versus K and choose the approriate value of k\n",
        "- Plot the silhouette_score versus K and use it to choose the best K\n",
        "- Store the silhouette_score for the best K for later comparison with other clustering techniques.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### plot and display k means with 10 k values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ne3KmtPudb9"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 5, figsize=(30, 12))\n",
        "\n",
        "i, j = 0, 0\n",
        "clust_range = range(1, 11)\n",
        "for num_clust in clust_range:\n",
        "\n",
        "    if i == 3:\n",
        "        break\n",
        "    if j == 5:\n",
        "        j = 0\n",
        "        i += 1\n",
        "\n",
        "    # get K means\n",
        "    kmeans = KMeans(n_clusters=num_clust)\n",
        "    kmeans.fit(Multi_blob_Data)\n",
        "    y_kmeans = kmeans.predict(Multi_blob_Data)\n",
        "\n",
        "    ax[i][j].scatter(\n",
        "        Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=y_kmeans, s=20, cmap=\"hsv\"\n",
        "    )\n",
        "    # draw centers\n",
        "    centers = kmeans.cluster_centers_\n",
        "    ax[i][j].scatter(centers[:, 0], centers[:, 1], c=\"black\", s=100, alpha=0.5)\n",
        "\n",
        "    # did not use display cluster as color list only has the seven single letter built in cluster\n",
        "    # the method I used here is almost the same but working\n",
        "\n",
        "    # set title for each graph\n",
        "\n",
        "    ax[i][j].set_title(f\"K-means with {num_clust} clusters\")\n",
        "    ttl = ax[i][j].title\n",
        "    ttl.set_position([0.5, 2])\n",
        "\n",
        "    j += 1\n",
        "\n",
        "fig.suptitle(\"K-means for multi blob data\")\n",
        "fig.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Distortion function vs K\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "i, j = 0, 0\n",
        "inertia = []\n",
        "distortions = []\n",
        "silhouette_avg = []\n",
        "for num_clust in clust_range:\n",
        "\n",
        "    # get K means\n",
        "    kmeans = KMeans(n_clusters=num_clust)\n",
        "    kmeans.fit(Multi_blob_Data)\n",
        "\n",
        "    cluster_labels = kmeans.labels_\n",
        "\n",
        "    inertia.append(kmeans.inertia_)\n",
        "    distortions.append(\n",
        "        sum(\n",
        "            np.min(cdist(Multi_blob_Data, kmeans.cluster_centers_, \"euclidean\"), axis=1)\n",
        "        )\n",
        "        / Multi_blob_Data.shape[0]\n",
        "    )\n",
        "\n",
        "    # silhouette score\n",
        "    if num_clust != 1:\n",
        "        silhouette_avg.append(silhouette_score(Multi_blob_Data, cluster_labels))\n",
        "\n",
        "plt.plot(clust_range, inertia, \"bx-\")\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Sum_of_squared_distances\")\n",
        "plt.title(\"Elbow Method For Optimal k\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Find silhouette score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# silhouette score\n",
        "plt.plot(range(2, 11), silhouette_avg, \"bx-\")\n",
        "plt.xlabel(\"Values of K\")\n",
        "plt.ylabel(\"Silhouette score\")\n",
        "plt.title(\"Silhouette analysis For Optimal k\")\n",
        "plt.show()\n",
        "\n",
        "# +2 because range starts from 2 as we removed one cluster as it does not work on silhouette score\n",
        "print(\"best K is\", pd.Series(silhouette_avg).idxmax() + 2)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kE7dvpOAudb9"
      },
      "source": [
        "### Hierarchal Clustering\n",
        "\n",
        "- Use AgglomerativeClustering function to to cluster the above data\n",
        "- In the AgglomerativeClustering change the following parameters\n",
        "  - Affinity (use euclidean, manhattan and cosine)\n",
        "  - Linkage( use average and single )\n",
        "  - Distance_threshold (try different)\n",
        "- For each of these trials plot the Dendograph , calculate the silhouette_score and display the resulting clusters\n",
        "- Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques.\n",
        "- Record your observation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dendrogram to help with number of clusters\n",
        "fig, ax = plt.subplots(1, 3, figsize=(30, 12))\n",
        "\n",
        "# average\n",
        "ax[0].set_title(\"Dendrograms for average\")\n",
        "dend_avg = hierarchy.dendrogram(\n",
        "    hierarchy.linkage(Multi_blob_Data, method=\"average\"), ax=ax[0]\n",
        ")\n",
        "ax[0].axhline(y=5, color=\"r\", linestyle=\"--\")\n",
        "# single\n",
        "ax[1].set_title(\"Dendrograms for single\")\n",
        "dend_single = hierarchy.dendrogram(\n",
        "    hierarchy.linkage(Multi_blob_Data, method=\"single\"), ax=ax[1]\n",
        ")\n",
        "ax[1].axhline(y=5, color=\"r\", linestyle=\"--\")\n",
        "# ward\n",
        "ax[2].set_title(\"Dendrograms for ward\")\n",
        "dend_single = hierarchy.dendrogram(\n",
        "    hierarchy.linkage(Multi_blob_Data, method=\"ward\"), ax=ax[2]\n",
        ")\n",
        "ax[2].axhline(y=100, color=\"r\", linestyle=\"--\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Takeaways based on dendrogram\n",
        "\n",
        "- Average is better than single so it will be used\n",
        "- After using the dendrogram for average and drawing the threshold line clusters number will be 5\n",
        "- this is a general guideline we will use distance threshold in this example\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3O_6WwKoudb-"
      },
      "outputs": [],
      "source": [
        "hierarchal_range = range(1, 25)\n",
        "affinity = [\"euclidean\", \"manhattan\", \"cosine\"]\n",
        "distance_threshold = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "fig, axes = plt.subplots(3, 8, figsize=(40, 20))\n",
        "i, j = 0, 0\n",
        "\n",
        "\n",
        "for aggloClusterGraph in hierarchal_range:\n",
        "\n",
        "    if i == 3:\n",
        "        break\n",
        "    if j == 8:\n",
        "        j = 0\n",
        "        i += 1\n",
        "\n",
        "    clustering_model = AgglomerativeClustering(\n",
        "        n_clusters=None,\n",
        "        affinity=affinity[i],\n",
        "        linkage=\"average\",\n",
        "        distance_threshold=distance_threshold[j],\n",
        "    )\n",
        "    # clustering_model.fit(Multi_blob_Data)\n",
        "    HClusteringY = clustering_model.fit_predict(Multi_blob_Data)\n",
        "\n",
        "    labels = clustering_model.labels_\n",
        "\n",
        "    axes[i][j].scatter(\n",
        "        Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=HClusteringY, cmap=\"tab10\"\n",
        "    )\n",
        "\n",
        "    if np.any(HClusteringY.T):\n",
        "        # get centroids\n",
        "        clf = NearestCentroid()\n",
        "        # print(HClusteringY)\n",
        "        clf.fit(Multi_blob_Data, HClusteringY)\n",
        "        axes[i][j].scatter(\n",
        "            clf.centroids_[:, 0],\n",
        "            clf.centroids_[:, 1],\n",
        "            c=\"black\",\n",
        "            s=100,\n",
        "            alpha=0.5,\n",
        "            marker=\"x\",\n",
        "        )\n",
        "\n",
        "    # set title for each graph\n",
        "    axes[i][j].set_title(\n",
        "        f\"Affinity {affinity[i]} and Distance threshold {distance_threshold[j]}\"\n",
        "    )\n",
        "    ttl = axes[i][j].title\n",
        "    ttl.set_position([0.5, 2])\n",
        "\n",
        "    j += 1\n",
        "\n",
        "\n",
        "fig.suptitle(\"Agglomerative clustering for multi blob data\")\n",
        "fig.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some Takeaways\n",
        "\n",
        "- single is terrible all the results are mostly 1 cluster with some outliers\n",
        "- cosine affinity is not suitable at all for this dataset\n",
        "- distance threshold from 4 to 6 should only be considered as out of this range clusters are too many or too few\n",
        "\n",
        "All the results for single and cosine with average linkage was combined in the code below as they are unimportant they are only for reference\n",
        "we will run the same tests again but using the new range for better results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# same code as above but with single and cosine average\n",
        "hierarchal_range = range(1, 29)\n",
        "affinity = [\"euclidean\", \"manhattan\", \"cosine\", \"cosine\"]\n",
        "linkage = [\"single\", \"single\", \"single\", \"average\"]\n",
        "distance_threshold = [1, 2, 3, 4, 5, 6, 7, 8]\n",
        "fig, axes = plt.subplots(4, 7, figsize=(45, 25))\n",
        "i, j = 0, 0\n",
        "\n",
        "\n",
        "for aggloClusterGraph in hierarchal_range:\n",
        "\n",
        "    if i == 4:\n",
        "        break\n",
        "    if j == 7:\n",
        "        j = 0\n",
        "        i += 1\n",
        "\n",
        "    clustering_model = AgglomerativeClustering(\n",
        "        n_clusters=None,\n",
        "        affinity=affinity[i],\n",
        "        linkage=linkage[i],\n",
        "        distance_threshold=distance_threshold[j],\n",
        "    )\n",
        "    HClusteringY = clustering_model.fit_predict(Multi_blob_Data)\n",
        "    # labels = clustering_model.labels_\n",
        "\n",
        "    axes[i][j].scatter(\n",
        "        Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=HClusteringY, cmap=\"bwr\"\n",
        "    )\n",
        "\n",
        "    if np.any(HClusteringY.T):\n",
        "        # get centroids\n",
        "        clf = NearestCentroid()\n",
        "        # print(HClusteringY)\n",
        "        clf.fit(Multi_blob_Data, HClusteringY)\n",
        "        axes[i][j].scatter(\n",
        "            clf.centroids_[:, 0],\n",
        "            clf.centroids_[:, 1],\n",
        "            c=\"black\",\n",
        "            s=100,\n",
        "            alpha=0.5,\n",
        "            marker=\"x\",\n",
        "        )\n",
        "\n",
        "    # set title for each graph\n",
        "    axes[i][j].set_title(\n",
        "        f\"affinity {affinity[i]} and distance threshold {distance_threshold[j]}\"\n",
        "    )\n",
        "    if i == 3:\n",
        "        axes[i][j].set_title(\n",
        "            f\"affinity {affinity[i]} and distance threshold {distance_threshold[j]} (average)\"\n",
        "        )\n",
        "    ttl = axes[i][j].title\n",
        "    ttl.set_position([0.5, 2])\n",
        "\n",
        "    j += 1\n",
        "\n",
        "\n",
        "fig.suptitle(\"Agglomerative clustering for multi blob data using single linkage\")\n",
        "fig.tight_layout()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Try Agglomerative clustering with new range (4 to 6)\n",
        "\n",
        "manhattan from 6 to 8 was tried as well\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hierarchal_range = range(1, 22)\n",
        "affinity = [\"euclidean\", \"manhattan\", \"manhattan\"]\n",
        "distance_threshold = [4, 4.5, 5, 5.25, 5.5, 5.75, 6]\n",
        "distance_threshold_manhattan = [6.25, 6.5, 6.75, 7, 7.25, 7.5, 8]\n",
        "fig, axes = plt.subplots(3, 7, figsize=(40, 15))\n",
        "\n",
        "\n",
        "silhouette_avg = []\n",
        "i, j = 0, 0\n",
        "for aggloClusterGraph in hierarchal_range:\n",
        "\n",
        "    if i == 3:\n",
        "        break\n",
        "    if j == 7:\n",
        "        j = 0\n",
        "        i += 1\n",
        "    if i == 2:\n",
        "        distance_threshold = distance_threshold_manhattan\n",
        "    clustering_model = AgglomerativeClustering(\n",
        "        n_clusters=None,\n",
        "        affinity=affinity[i],\n",
        "        linkage=\"average\",\n",
        "        distance_threshold=distance_threshold[j],\n",
        "    )\n",
        "    # clustering_model.fit(Multi_blob_Data)\n",
        "    HClusteringY = clustering_model.fit_predict(Multi_blob_Data)\n",
        "    labels = clustering_model.labels_\n",
        "\n",
        "    axes[i][j].scatter(\n",
        "        Multi_blob_Data[:, 0],\n",
        "        Multi_blob_Data[:, 1],\n",
        "        c=HClusteringY,\n",
        "        cmap=\"gist_rainbow\",\n",
        "    )\n",
        "\n",
        "    # to get centroids T returns a array if all is zero then only one cluster is present\n",
        "    # which means no getting centroids as having one cluster only causes an error in the function\n",
        "    if np.any(HClusteringY.T):\n",
        "        # get centroids\n",
        "        clf = NearestCentroid()\n",
        "        # print(HClusteringY)\n",
        "        clf.fit(Multi_blob_Data, HClusteringY)\n",
        "        axes[i][j].scatter(\n",
        "            clf.centroids_[:, 0],\n",
        "            clf.centroids_[:, 1],\n",
        "            c=\"black\",\n",
        "            s=100,\n",
        "            alpha=0.5,\n",
        "            marker=\"x\",\n",
        "        )\n",
        "\n",
        "    # silhouette score\n",
        "    silhouette_avg.append(silhouette_score(Multi_blob_Data, labels))\n",
        "\n",
        "    # set title for each graph\n",
        "    axes[i][j].set_title(\n",
        "        f\"Affinity {affinity[i]} and Distance threshold {distance_threshold[j]}\"\n",
        "    )\n",
        "    ttl = axes[i][j].title\n",
        "    ttl.set_position([0.5, 2])\n",
        "\n",
        "    j += 1\n",
        "\n",
        "\n",
        "fig.suptitle(\n",
        "    \"Second Agglomerative clustering for multi blob data with a confined range of 4 to 6\"\n",
        ")\n",
        "fig.tight_layout()\n",
        "\n",
        "# used in next step\n",
        "distance_threshold = [4, 4.5, 5, 5.25, 5.5, 5.75, 6]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# silhouette score\n",
        "plt.plot(range(0, 7), silhouette_avg[0:7], \"bx-\", label=\"euclidean 4 to 6\", color=\"r\")\n",
        "plt.plot(range(7, 14), silhouette_avg[7:14], \"bx-\", label=\"manhattan 4 to 6\", color=\"b\")\n",
        "plt.plot(\n",
        "    range(14, 21), silhouette_avg[14:21], \"bx-\", label=\"manhattan 6 to 8\", color=\"g\"\n",
        ")\n",
        "plt.xlabel(\"Values of distance and affinity\")\n",
        "plt.ylabel(\"Silhouette score\")\n",
        "plt.title(\"Silhouette analysis For optimal Agglomerative clustering\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()\n",
        "\n",
        "optimal_index = pd.Series(silhouette_avg).idxmax()\n",
        "opt_div = optimal_index // 7\n",
        "affinity = \"\"\n",
        "\n",
        "if optimal_index < 14:\n",
        "    distance_thresh = distance_threshold[optimal_index % 7]\n",
        "else:\n",
        "    distance_thresh = distance_threshold_manhattan[optimal_index % 7]\n",
        "\n",
        "if optimal_index < 7:\n",
        "    affinity = \"euclidean\"\n",
        "else:\n",
        "    affinity = \"manhattan\"\n",
        "\n",
        "\n",
        "print(\n",
        "    f\"best Agglomerative clustering is at index {optimal_index} which is {affinity} affinity with distance threshold of {distance_thresh}\"\n",
        ")\n",
        "print(distance_threshold[1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Summary till now:\n",
        "\n",
        "#### Kmeans\n",
        "\n",
        "best K is at 6\n",
        "\n",
        "#### Agglomerative clustering\n",
        "\n",
        "best parameters are euclidean affinity with distance threshold of 4.5\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "myJE7vQKudb-"
      },
      "source": [
        "### DBScan\n",
        "\n",
        "- Use DBScan function to to cluster the above data\n",
        "- In the DBscan change the following parameters\n",
        "  - EPS (from 0.1 to 3)\n",
        "  - Min_samples (from 5 to 25)\n",
        "- Plot the silhouette_score versus the variation in the EPS and the min_samples\n",
        "- Plot the resulting Clusters in this case\n",
        "- Find the set of paramters that would find result in the best silhouette_score and store this score for later comparison with other clustering techniques.\n",
        "- Record your observations and comments\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute data proximity from each other using Nearest Neighbours\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "neighb = NearestNeighbors(\n",
        "    n_neighbors=5\n",
        ")  # creating an object of the NearestNeighbors class\n",
        "nbrs = neighb.fit(Multi_blob_Data)  # fitting the data to the object\n",
        "distances, indices = nbrs.kneighbors(Multi_blob_Data)  # finding the nearest neighbours\n",
        "\n",
        "distances = np.sort(distances, axis=0)  # sorting the distances\n",
        "distances = distances[:, 1]  # taking the second column of the sorted distances\n",
        "plt.rcParams[\"figure.figsize\"] = (5, 3)  # setting the figure size\n",
        "plt.plot(distances)  # plotting the distances\n",
        "plt.show()  # showing the plot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "after plotting the main graph we find that the maximum curvature is at eps = 0.5 so this should be the best eps to use\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiQtpAt5udb_"
      },
      "outputs": [],
      "source": [
        "df_Multi_blob = Multi_blob_Data\n",
        "\n",
        "# clustering function\n",
        "def DB_Clustering(eps, num_sample):\n",
        "    DBSCAN_Clustering_model = DBSCAN(eps=eps, min_samples=num_sample).fit(\n",
        "        Multi_blob_Data\n",
        "    )\n",
        "    return DBSCAN_Clustering_model.labels_\n",
        "\n",
        "\n",
        "def DBSCAN_Subplots(nrows, ncols, inverse, eps_max, min_samples_max):\n",
        "    # create subplots and ranges\n",
        "    num_Graphs = nrows * ncols\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 12, nrows * 10))\n",
        "    eps = np.linspace(0.1, eps_max, num=num_Graphs)\n",
        "    if inverse:\n",
        "        min_samples = np.linspace(min_samples_max, 5, num=num_Graphs)\n",
        "    else:\n",
        "        min_samples = np.linspace(5, min_samples_max, num=num_Graphs)\n",
        "    DBSCAN_Range = range(0, num_Graphs)\n",
        "\n",
        "    silhouette_avg = []\n",
        "    i, j = 0, 0\n",
        "    for DBSCAN_Graph in DBSCAN_Range:\n",
        "        if i == nrows:\n",
        "            break\n",
        "        if j == ncols:\n",
        "            j = 0\n",
        "            i += 1\n",
        "\n",
        "        DBSCAN_label = DB_Clustering(eps[DBSCAN_Graph], min_samples[DBSCAN_Graph])\n",
        "        axes[i][j].scatter(\n",
        "            x=df_Multi_blob[:, 0],\n",
        "            y=df_Multi_blob[:, 1],\n",
        "            c=DBSCAN_label,\n",
        "            cmap=\"Accent\",\n",
        "            s=50,\n",
        "        )\n",
        "\n",
        "        # silhouette score\n",
        "        # if to check if the label array is all equal, if all equal then only one cluster is present which throws an error in silhouette function\n",
        "        if len(np.unique(DBSCAN_label)) > 1:\n",
        "            silhouette_avg.append(\n",
        "                {\n",
        "                    \"value\": silhouette_score(Multi_blob_Data, DBSCAN_label),\n",
        "                    \"eps\": eps[DBSCAN_Graph],\n",
        "                    \"min_samples\": min_samples[DBSCAN_Graph],\n",
        "                }\n",
        "            )\n",
        "\n",
        "            # print(silhouette_avg[-1],DBSCAN_Graph)\n",
        "\n",
        "        # set title for each graph\n",
        "        axes[i][j].set_title(\n",
        "            f\"DBSCAN with eps of {round(eps[DBSCAN_Graph],3)} and minimum samples of {round(min_samples[DBSCAN_Graph],3)}\"\n",
        "        )\n",
        "        ttl = axes[i][j].title\n",
        "        ttl.set_position([0.5, 2])\n",
        "\n",
        "        j += 1\n",
        "    fig.tight_layout()\n",
        "    fig.suptitle(\n",
        "        f\"DBSCAN clustering for multi blob data with using {num_Graphs} graphs and eps from 0.1 to {eps_max} and minimum samples from 5 to {min_samples_max}\",\n",
        "        y=1.02,\n",
        "    )\n",
        "    return silhouette_avg\n",
        "\n",
        "\n",
        "# DBSCAN_Subplots(4,4,False)\n",
        "silhouette_avg = DBSCAN_Subplots(5, 5, True, 3, 25)\n",
        "silhouette_avg2 = DBSCAN_Subplots(5, 5, False, 3, 25)\n",
        "\n",
        "# for testing\n",
        "# plt.scatter(x=df_Multi_blob[:,0],y=df_Multi_blob[:,1],c=DB_Clustering(0.4,5),cmap=\"hsv\",s=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "silhouette_avg3 = DBSCAN_Subplots(5, 5, True, 2, 25)\n",
        "silhouette_avg4 = DBSCAN_Subplots(5, 5, False, 2, 25)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After performing the first dbscan wave it is time to narrow down the parameters\n",
        "\n",
        "the new eps range is 0.1 to 1.1\n",
        "\n",
        "the new min samples range is the same\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "silhouette_avg5 = DBSCAN_Subplots(5, 5, True, 1.1, 25)\n",
        "silhouette_avg6 = DBSCAN_Subplots(5, 5, False, 1.1, 25)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_silhouette_avg(\n",
        "    silhouette_avg,\n",
        "    silhouette_avg_values,\n",
        "    silhouette_avg_eps,\n",
        "    silhouette_avg_min_samples,\n",
        "    all_silhouette_avg,\n",
        "):\n",
        "    for idx, dicts in enumerate(silhouette_avg):\n",
        "        silhouette_avg_values.append(silhouette_avg[idx][\"value\"])\n",
        "        silhouette_avg_eps.append(silhouette_avg[idx][\"eps\"])\n",
        "        silhouette_avg_min_samples.append(silhouette_avg[idx][\"min_samples\"])\n",
        "        all_silhouette_avg.append(silhouette_avg[idx])\n",
        "\n",
        "\n",
        "silhouette_avg_values = []\n",
        "silhouette_avg_eps = []\n",
        "silhouette_avg_min_samples = []\n",
        "all_silhouette_avg = []\n",
        "\n",
        "get_silhouette_avg(\n",
        "    silhouette_avg,\n",
        "    silhouette_avg_values,\n",
        "    silhouette_avg_eps,\n",
        "    silhouette_avg_min_samples,\n",
        "    all_silhouette_avg,\n",
        ")\n",
        "get_silhouette_avg(\n",
        "    silhouette_avg2,\n",
        "    silhouette_avg_values,\n",
        "    silhouette_avg_eps,\n",
        "    silhouette_avg_min_samples,\n",
        "    all_silhouette_avg,\n",
        ")\n",
        "get_silhouette_avg(\n",
        "    silhouette_avg3,\n",
        "    silhouette_avg_values,\n",
        "    silhouette_avg_eps,\n",
        "    silhouette_avg_min_samples,\n",
        "    all_silhouette_avg,\n",
        ")\n",
        "get_silhouette_avg(\n",
        "    silhouette_avg4,\n",
        "    silhouette_avg_values,\n",
        "    silhouette_avg_eps,\n",
        "    silhouette_avg_min_samples,\n",
        "    all_silhouette_avg,\n",
        ")\n",
        "get_silhouette_avg(\n",
        "    silhouette_avg5,\n",
        "    silhouette_avg_values,\n",
        "    silhouette_avg_eps,\n",
        "    silhouette_avg_min_samples,\n",
        "    all_silhouette_avg,\n",
        ")\n",
        "get_silhouette_avg(\n",
        "    silhouette_avg6,\n",
        "    silhouette_avg_values,\n",
        "    silhouette_avg_eps,\n",
        "    silhouette_avg_min_samples,\n",
        "    all_silhouette_avg,\n",
        ")\n",
        "\n",
        "# print(len(silhouette_avg_values),len(silhouette_avg_eps),len(silhouette_avg_min_samples))\n",
        "optimal_index = pd.Series(silhouette_avg_values).idxmax()\n",
        "optimal_eps = silhouette_avg_eps[optimal_index]\n",
        "optimal_min_samples = silhouette_avg_min_samples[optimal_index]\n",
        "\n",
        "print(f\"best eps is {optimal_eps} ,best min_samples is {optimal_min_samples}\")\n",
        "\n",
        "# silhouette score\n",
        "plt.plot(range(1, len(all_silhouette_avg) + 1), silhouette_avg_values, color=\"r\")\n",
        "plt.xlabel(\"index of dbscan\")\n",
        "plt.ylabel(\"Silhouette score\")\n",
        "plt.title(\"Silhouette analysis For optimal Agglomerative clustering\")\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip16g1QFudb_"
      },
      "source": [
        "### Gaussian Mixture\n",
        "\n",
        "- Use GaussianMixture function to cluster the above data\n",
        "- In GMM change the covariance_type and check the difference in the resulting proabability fit\n",
        "- Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a list of n_components values\n",
        "n_components = list(range(1, 21))\n",
        "\n",
        "fig, axes = plt.subplots(5, 4, figsize=(18, 22))\n",
        "\n",
        "# Loop over the n_components values\n",
        "for n, ax in zip(n_components, axes.ravel()):\n",
        "    # Create a GaussianMixture object with n components and fit model\n",
        "    gm = GaussianMixture(n_components=n).fit(Multi_blob_Data)\n",
        "    # Predict the labels for each data point\n",
        "    labels = gm.predict(Multi_blob_Data)\n",
        "    # Plot the data points with different colors according to their labels\n",
        "    ax.scatter(\n",
        "        Multi_blob_Data[:, 0], Multi_blob_Data[:, 1], c=labels, cmap=\"gist_rainbow\"\n",
        "    )\n",
        "\n",
        "    # Add a title with the number of components\n",
        "    ax.set_title(f\"GMM with {n} components\")\n",
        "# Adjust the spacing between subplots\n",
        "fig.tight_layout()\n",
        "# Show the figure\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Gaussian mixture with different covariance types and with 2d contour plot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = Multi_blob_Data\n",
        "\n",
        "# Define the range of components and covariance types\n",
        "n_components_range = range(1, 21)\n",
        "covariance_types = [\"full\", \"tied\", \"diag\", \"spherical\"]\n",
        "\n",
        "# Loop over the covariance types\n",
        "for i, cov_type in enumerate(covariance_types):\n",
        "    # Create a figure with 20 subplots (4 rows and 5 columns) for each covariance type\n",
        "    fig, axes = plt.subplots(4, 5, figsize=(35, 28))\n",
        "    # Loop over the range of components\n",
        "    for j, n_components in enumerate(n_components_range):\n",
        "        # Create a GMM object with the given parameters\n",
        "        gmm = GaussianMixture(\n",
        "            n_components=n_components, covariance_type=cov_type, random_state=0\n",
        "        )\n",
        "        # Fit the GMM to the data\n",
        "        gmm.fit(X)\n",
        "        # Predict the labels for each data point\n",
        "        labels = gmm.predict(X)\n",
        "        # Compute the log probability density of each data point under the model\n",
        "        log_prob = gmm.score_samples(X)\n",
        "\n",
        "        # Plot the data points with different colors for each label on the corresponding subplot\n",
        "        axes[j // 5][j % 5].scatter(X[:, 0], X[:, 1], c=labels, s=10, cmap=\"hsv\")\n",
        "\n",
        "        # Set up a grid of points for plotting contours\n",
        "        x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "        y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "        xx, yy = np.meshgrid(\n",
        "            np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100)\n",
        "        )\n",
        "        xy = np.column_stack([xx.ravel(), yy.ravel()])\n",
        "\n",
        "        # Compute and plot contours of probability density on top of data points\n",
        "        z_log_prob = gmm.score_samples(xy).reshape(xx.shape)\n",
        "        axes[j // 5][j % 5].contour(xx, yy, z_log_prob)\n",
        "\n",
        "        axes[j // 5][j % 5].set_title(\n",
        "            f\"GMM with {cov_type} covariance and {n_components} components\",\n",
        "            fontsize=16,\n",
        "            y=1.05,\n",
        "        )\n",
        "        axes[j // 5][j % 5].set_xlabel(\"x\")\n",
        "        axes[j // 5][j % 5].set_ylabel(\"y\")\n",
        "        ttl = axes[j // 5][j % 5].title\n",
        "        ttl.set_position([0.5, 1])\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "plt.tight_layout()\n",
        "# Show the figure\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m92lZkkyudb_"
      },
      "source": [
        "## iris data set\n",
        "\n",
        "The iris data set is test data set that is part of the Sklearn module\n",
        "which contains 150 records each with 4 features. All the features are represented by real numbers\n",
        "\n",
        "The data represents three classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QaCWyyCudcA",
        "outputId": "79c14dba-80cf-4d96-e69d-70763b789faf"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris_data = load_iris()\n",
        "iris_data.target[[10, 25, 50]]\n",
        "# array([0, 0, 1])\n",
        "list(iris_data.target_names)\n",
        "[\"setosa\", \"versicolor\", \"virginica\"]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K means clustering on iris dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "\n",
        "def kmeans_iris(x, y):\n",
        "    fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
        "    axs = axs.ravel()\n",
        "\n",
        "    sil_scores = []\n",
        "    distortion = []\n",
        "\n",
        "    for i in range(1, 11):\n",
        "        kmeans = KMeans(n_clusters=i)\n",
        "        kmeans.fit(X)\n",
        "        labels = kmeans.labels_\n",
        "        centers = kmeans.cluster_centers_\n",
        "        distortion.append(kmeans.inertia_)\n",
        "\n",
        "        if i > 1:\n",
        "            sil_score = silhouette_score(X, labels)\n",
        "            sil_scores.append(sil_score)\n",
        "            axs[i - 1].set_title(f\"k={i}, silhouette={sil_score:.2f}\", y=1.01)\n",
        "        else:\n",
        "            axs[i - 1].set_title(f\"k={i}\", y=1.01)\n",
        "\n",
        "        # plot cluster and centers\n",
        "        axs[i - 1].scatter(X[:, x], X[:, y], c=labels, cmap=\"gist_rainbow\")\n",
        "        axs[i - 1].scatter(\n",
        "            centers[:, x], centers[:, y], marker=\"o\", s=100, color=\"black\"\n",
        "        )\n",
        "\n",
        "        if x == 0 and y == 1:\n",
        "            axs[i - 1].set_xlabel(\"sepal_length\")\n",
        "            axs[i - 1].set_ylabel(\"sepal_width\")\n",
        "        else:\n",
        "            axs[i - 1].set_xlabel(\"petal_length\")\n",
        "            axs[i - 1].set_ylabel(\"petal_width\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    return distortion, sil_scores\n",
        "\n",
        "\n",
        "distortion, sil_scores = kmeans_iris(0, 1)\n",
        "kmeans_iris(2, 3)\n",
        "\n",
        "\n",
        "# create elbow graph\n",
        "plt.figure()\n",
        "plt.plot(range(1, 11), distortion)\n",
        "plt.title(\"The elbow method\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"SUm of squares distance\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "# create and plot sil graph\n",
        "plt.figure()\n",
        "plt.plot(range(2, 11), sil_scores)\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the elbow method k=3 is the best solution\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchal Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "\n",
        "def aggro_hierarchal_iris(dist_min, dist_max):\n",
        "    affinities = [\"euclidean\", \"manhattan\", \"cosine\"]\n",
        "    linkages = [\"average\", \"single\"]\n",
        "    distance_thresholds = np.linspace(dist_min, dist_max, num=16)\n",
        "\n",
        "    # fig, axs = plt.subplots(len(affinities) * len(linkages), len(distance_thresholds), figsize=(50, 25))\n",
        "\n",
        "    best_score = -1\n",
        "    best_params = []\n",
        "    sil_score = []\n",
        "\n",
        "    for i, affinity in enumerate(affinities):\n",
        "        for j, linkage in enumerate(linkages):\n",
        "\n",
        "            fig, ax = plt.subplots(4, 4, figsize=(30, 20))\n",
        "            ax = ax.ravel()\n",
        "\n",
        "            for k, distance_threshold in enumerate(distance_thresholds):\n",
        "                # Create an instance of AgglomerativeClustering with desired parameters\n",
        "                cluster = AgglomerativeClustering(\n",
        "                    n_clusters=None,\n",
        "                    affinity=affinity,\n",
        "                    linkage=linkage,\n",
        "                    distance_threshold=distance_threshold,\n",
        "                )\n",
        "\n",
        "                # Fit the model to the data\n",
        "                cluster.fit(X)\n",
        "\n",
        "                # Get the cluster labels for each data point\n",
        "                labels = cluster.labels_\n",
        "\n",
        "                score = 0\n",
        "                # Calculate silhouette score\n",
        "                if len(np.unique(labels.T)) > 1:\n",
        "                    score = silhouette_score(X, labels)\n",
        "                    sil_score.append(score)\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = (affinity, linkage, distance_threshold)\n",
        "\n",
        "                ax[k].scatter(X[:, 0], X[:, 1], c=labels, cmap=\"rainbow\")\n",
        "                ax[k].set_title(\n",
        "                    f\"Distance Threshold: {distance_threshold:.2f}\\nSilhouette Score: {score:.2f}\"\n",
        "                )\n",
        "\n",
        "                ax[k].set_xlabel(\"sepal_length\")\n",
        "                ax[k].set_ylabel(\"sepal_width\")\n",
        "\n",
        "                if np.any(labels.T):\n",
        "                    # get centroids\n",
        "                    clf = NearestCentroid()\n",
        "                    # print(HClusteringY)\n",
        "                    clf.fit(X, labels)\n",
        "                    ax[k].scatter(\n",
        "                        clf.centroids_[:, 0],\n",
        "                        clf.centroids_[:, 1],\n",
        "                        c=\"black\",\n",
        "                        s=100,\n",
        "                        alpha=0.5,\n",
        "                        marker=\"x\",\n",
        "                    )\n",
        "\n",
        "            fig.suptitle(\n",
        "                f\"Agglomerative clustering with Affinity: {affinity} and Linkage: {linkage}\"\n",
        "            )\n",
        "\n",
        "            fig.tight_layout()\n",
        "            plt.show()\n",
        "    return best_params, sil_score\n",
        "\n",
        "\n",
        "best_params, sil_score = aggro_hierarchal_iris(0.1, 5)\n",
        "\n",
        "# create silhouette graph\n",
        "plt.figure()\n",
        "plt.plot(range(len(sil_score)), sil_score)\n",
        "plt.title(\"silhouette score\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "print(\n",
        "    f\"Best parameters: Affinity={best_params[0]}, Linkage={best_params[1]}, Distance Threshold={best_params[2]}\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DBScan\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "col = [\"sepal length\", \"sepal width\", \"petal length\", \"petal width\"]\n",
        "\n",
        "\n",
        "def DBSCAN_iris(x, y):\n",
        "    eps_values = np.linspace(0.1, 3.0, num=30)\n",
        "    min_samples_values = np.linspace(5, 25, num=30)\n",
        "\n",
        "    fig, axs = plt.subplots(6, 5, figsize=(35, 35))\n",
        "\n",
        "    x_label = col[x]\n",
        "    y_label = col[y]\n",
        "\n",
        "    best_silhouette_score = -1\n",
        "    best_eps = None\n",
        "    best_min_samples = None\n",
        "    sil_scores = []\n",
        "\n",
        "    ax = axs.ravel()\n",
        "\n",
        "    for i in range(len(eps_values)):\n",
        "        eps = eps_values[i]\n",
        "        min_samples = min_samples_values[i]\n",
        "\n",
        "        db = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
        "        labels = db.labels_\n",
        "\n",
        "        ax[i].scatter(X[:, x], X[:, y], c=labels, cmap=\"gist_rainbow\")\n",
        "        ax[i].set_title(f\"eps={eps:.2f}, min_samples={min_samples}\")\n",
        "        ax[i].set_xlabel(f\"{x_label}\")\n",
        "        ax[i].set_ylabel(f\"{y_label}\")\n",
        "\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "        if n_clusters > 1:\n",
        "            silhouette_avg = silhouette_score(X, labels)\n",
        "            sil_scores.append(silhouette_avg)\n",
        "\n",
        "            if silhouette_avg > best_silhouette_score:\n",
        "                best_silhouette_score = silhouette_avg\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "\n",
        "    fig.suptitle(f\"DB SCAN of {x_label} versus {y_label}\", y=1.01)\n",
        "    fig.tight_layout()\n",
        "    return best_eps, best_min_samples, sil_scores\n",
        "\n",
        "\n",
        "best_eps, best_min_samples, sil_scores = DBSCAN_iris(0, 1)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(len(sil_scores)), sil_scores)\n",
        "plt.title(\"silhouette score\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best eps: {best_eps:.2f}\")\n",
        "print(f\"Best min samples: {best_min_samples}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "db = DBSCAN(eps=0.99, min_samples=11.15).fit(X)\n",
        "labels = db.labels_\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
        "plt.title(f\"eps=0.99, min_samples=11.15\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GMM clustering\n",
        "\n",
        "- Use GaussianMixture function to cluster the above data\n",
        "- In GMM change the covariance_type and check the difference in the resulting proabability fit\n",
        "- Use a 2D contour plot to plot the resulting distribution (the components of the GMM) as well as the total Gaussian mixture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # we only take the first two features.\n",
        "\n",
        "\n",
        "def GMM_iris(X):\n",
        "    n_components_range = range(1, 21)\n",
        "    covariance_types = [\"spherical\", \"tied\", \"diag\", \"full\"]\n",
        "\n",
        "    for index, covariance_type in enumerate(covariance_types):\n",
        "        plt.figure(figsize=(30, 18))\n",
        "        for n_components in n_components_range:\n",
        "            gmm = GaussianMixture(\n",
        "                n_components=n_components, covariance_type=covariance_type\n",
        "            )\n",
        "            gmm.fit(X)\n",
        "\n",
        "            plt.subplot(4, 5, n_components)\n",
        "\n",
        "            x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
        "            y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
        "\n",
        "            xx, yy = np.meshgrid(np.linspace(x_min, x_max), np.linspace(y_min, y_max))\n",
        "            Z = gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "            Z = Z.reshape(xx.shape)\n",
        "            plt.contour(xx, yy, Z)\n",
        "\n",
        "            plt.scatter(X[:, 0], X[:, 1], c=gmm.predict(X), cmap=\"gist_rainbow\")\n",
        "            plt.title(f\"n_components={n_components}\")\n",
        "\n",
        "        plt.suptitle(f\"GMM with {covariance_type} covariance\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "GMM_iris(X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Repeat all steps after normalizing\n",
        "\n",
        "### K-means\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "X = prep.normalize(X)\n",
        "\n",
        "kmeans_iris(0, 1)\n",
        "kmeans_iris(2, 3)\n",
        "\n",
        "\n",
        "# create elbow graph\n",
        "plt.figure()\n",
        "plt.plot(range(1, 11), distortion)\n",
        "plt.title(\"The elbow method\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"SUm of squares distance\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "# create and plot sil graph\n",
        "plt.figure()\n",
        "plt.plot(range(2, 11), sil_scores)\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.show()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchial Clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "X = prep.normalize(X)\n",
        "\n",
        "best_params, sil_score = aggro_hierarchal_iris(0.01, 1)\n",
        "\n",
        "# create silhouette graph\n",
        "plt.figure()\n",
        "plt.plot(range(len(sil_score)), sil_score)\n",
        "plt.title(\"silhouette score\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "print(\n",
        "    f\"Best parameters: Affinity={best_params[0]}, Linkage={best_params[1]}, Distance Threshold={best_params[2]}\"\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DBSCAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "X = prep.normalize(X)\n",
        "\n",
        "\n",
        "best_eps_petal, best_min_samples_petal, sil_score_petal = DBSCAN_iris(0, 1)\n",
        "best_eps_sepal, best_min_samples_sepal, sil_score_sepal = DBSCAN_iris(2, 3)\n",
        "\n",
        "# create silhouette graph\n",
        "plt.figure()\n",
        "plt.plot(range(len(sil_score_petal)), sil_score_petal)\n",
        "plt.title(\"silhouette score petals\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "# create silhouette graph\n",
        "plt.figure()\n",
        "plt.plot(range(len(sil_score_sepal)), sil_score_sepal)\n",
        "plt.title(\"silhouette score sepals\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "print(\n",
        "    f\"Best parameters for petals: eps={best_eps_petal}, min_samples={best_min_samples_petal}\"\n",
        ")\n",
        "print(\n",
        "    f\"Best parameters for sepals: eps={best_eps_sepal}, min_samples={best_min_samples_sepal}\"\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GMM clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # we only take the first two features.\n",
        "X = prep.normalize(X)\n",
        "\n",
        "\n",
        "def GMM_iris(X, label_flag):\n",
        "    n_components_range = range(1, 21)\n",
        "    covariance_types = [\"spherical\", \"tied\", \"diag\", \"full\"]\n",
        "\n",
        "    for index, covariance_type in enumerate(covariance_types):\n",
        "        plt.figure(figsize=(30, 18))\n",
        "        for n_components in n_components_range:\n",
        "            gmm = GaussianMixture(\n",
        "                n_components=n_components, covariance_type=covariance_type\n",
        "            )\n",
        "            gmm.fit(X)\n",
        "\n",
        "            plt.subplot(4, 5, n_components)\n",
        "\n",
        "            x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
        "            y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
        "\n",
        "            xx, yy = np.meshgrid(np.linspace(x_min, x_max), np.linspace(y_min, y_max))\n",
        "            Z = gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "            Z = Z.reshape(xx.shape)\n",
        "            plt.contour(xx, yy, Z)\n",
        "\n",
        "            plt.scatter(X[:, 0], X[:, 1], c=gmm.predict(X), cmap=\"gist_rainbow\")\n",
        "            plt.title(f\"n_components={n_components}\")\n",
        "\n",
        "            if label_flag == 0:\n",
        "                plt.xlabel(\"petal_length\")\n",
        "                plt.ylabel(\"petal_width\")\n",
        "            else:\n",
        "                plt.xlabel(\"sepal_length\")\n",
        "                plt.ylabel(\"sepal_width\")\n",
        "\n",
        "        plt.suptitle(\n",
        "            f'GMM with {covariance_type} covariance for {\"petals\" if label_flag==0 else \"Sepals\"}'\n",
        "        )\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "GMM_iris(X, 0)\n",
        "\n",
        "X = iris.data[:, 2:4]  # we take the second two features.\n",
        "X = prep.normalize(X)\n",
        "\n",
        "print(\"Now for sepal length and width\")\n",
        "\n",
        "GMM_iris(X, 1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WyoCVfyMudcA"
      },
      "source": [
        "- Repeat all the above clustering approaches and steps on the above data\n",
        "- Normalize the data then repeat all the above steps\n",
        "- Compare between the different clustering approaches\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N2oBmWT2udcA"
      },
      "source": [
        "## Customer dataset\n",
        "\n",
        "Repeat all the above on the customer data set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plotnine import *\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.datasets import make_blobs\n",
        "import sklearn.preprocessing as prep\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# StandardScaler is a function to normalize the data\n",
        "# You may also check MinMaxScaler and MaxAbsScaler\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# hierarchical clustering\n",
        "\n",
        "\n",
        "# for cdist in distortions graph\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_df = pd.read_csv(\"Customer data.csv\")\n",
        "print(customer_df.info())\n",
        "print(customer_df.head())\n",
        "print(customer_df.keys())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K means\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_df = pd.read_csv(\"Customer data.csv\")\n",
        "df = customer_df.drop(customer_df.columns[0], axis=1)\n",
        "scaler = prep.MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(df)\n",
        "X_dataframe = pd.DataFrame(normalized_data)\n",
        "X = pd.DataFrame.to_numpy(X_dataframe)\n",
        "\n",
        "# df = prep.normalize(df,axis=0)\n",
        "# df = pd.DataFrame(df)\n",
        "# print(df)\n",
        "# df.values[:,0] = df.values[:,0] +10\n",
        "# print(df)\n",
        "\n",
        "\n",
        "def kmeans_cust(x, y, df):\n",
        "    fig, axs = plt.subplots(2, 5, figsize=(20, 8))\n",
        "    axs = axs.ravel()\n",
        "\n",
        "    sil_scores = []\n",
        "    distortions = []\n",
        "\n",
        "    x_label = customer_df.columns[x+1]\n",
        "    y_label = customer_df.columns[y+1]\n",
        "\n",
        "    for i in range(0, 10):\n",
        "        kmeans = KMeans(n_clusters=i + 1)\n",
        "        kmeans.fit(df)\n",
        "        labels = kmeans.labels_\n",
        "        centers = kmeans.cluster_centers_\n",
        "        distortions.append(kmeans.inertia_)\n",
        "\n",
        "        if i > 0:\n",
        "            sil_score = silhouette_score(df, labels)\n",
        "            sil_scores.append(sil_score)\n",
        "            axs[i].set_title(f\"k={i}, silhouette={sil_score:.2f}\", y=1.01)\n",
        "        else:\n",
        "            axs[i].set_title(f\"k={i+1}\", y=1.01)\n",
        "\n",
        "        # plot cluster and centers\n",
        "        axs[i].scatter(df[:, x], df[:, y], c=labels, cmap=\"gist_rainbow\")\n",
        "        axs[i].scatter(centers[:, x], centers[:, y], marker=\"o\", s=100, color=\"black\")\n",
        "\n",
        "        axs[i].set_xlabel(f\"{x_label}\")\n",
        "        axs[i].set_ylabel(f\"{y_label}\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.suptitle(f\"K means clustering of {x_label} versus {y_label}\", y=1.03)\n",
        "    return sil_scores, distortions\n",
        "\n",
        "\n",
        "sil_scores, distortions = kmeans_cust(2, 4, X)\n",
        "for i in range(0, len(df.keys())):\n",
        "    if i != 2:\n",
        "        kmeans_cust(i, 2, X)\n",
        "\n",
        "for i in range(0, len(df.keys())):\n",
        "    if i != 4:\n",
        "        kmeans_cust(i, 4, X)\n",
        "    # kmeans_cust(7,5,X)\n",
        "# kmeans_cust(1,7,X)\n",
        "\n",
        "\n",
        "# create elbow graph\n",
        "plt.figure()\n",
        "plt.plot(range(len(distortions)), distortions)\n",
        "plt.title(\"The elbow method\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"SUm of squares distance\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "# create and plot sil graph\n",
        "plt.figure()\n",
        "plt.plot(range(len(sil_scores)), sil_scores)\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hierarchal clustering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# customer_df = pd.read_csv(\"Customer data.csv\")\n",
        "# df = customer_df.drop(customer_df.columns[0], axis=1)\n",
        "# X = prep.normalize(df)\n",
        "\n",
        "customer_df = pd.read_csv(\"Customer data.csv\")\n",
        "df = customer_df.drop(customer_df.columns[0], axis=1)\n",
        "scaler = prep.MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(df)\n",
        "X_dataframe = pd.DataFrame(normalized_data)\n",
        "X = pd.DataFrame.to_numpy(X_dataframe)\n",
        "\n",
        "\n",
        "def aggro_hierarchal_cust(dist_min, dist_max, x, y, df_cust):\n",
        "    affinities = [\"euclidean\", \"manhattan\", \"cosine\"]\n",
        "    linkages = [\"average\", \"single\"]\n",
        "    distance_thresholds = np.linspace(dist_min, dist_max, num=16)\n",
        "\n",
        "    # fig, axs = plt.subplots(len(affinities) * len(linkages), len(distance_thresholds), figsize=(50, 25))\n",
        "\n",
        "    best_score = -1\n",
        "    best_params = []\n",
        "    sil_score = []\n",
        "\n",
        "    x_label = customer_df.columns[x+1]\n",
        "    y_label = customer_df.columns[y+1]\n",
        "\n",
        "    for i, affinity in enumerate(affinities):\n",
        "        for j, linkage in enumerate(linkages):\n",
        "\n",
        "            fig, ax = plt.subplots(4, 4, figsize=(30, 20))\n",
        "            ax = ax.ravel()\n",
        "\n",
        "            for k, distance_threshold in enumerate(distance_thresholds):\n",
        "                # Create an instance of AgglomerativeClustering with desired parameters\n",
        "                cluster = AgglomerativeClustering(\n",
        "                    n_clusters=None,\n",
        "                    affinity=affinity,\n",
        "                    linkage=linkage,\n",
        "                    distance_threshold=distance_threshold,\n",
        "                )\n",
        "\n",
        "                # Fit the model to the data\n",
        "                cluster.fit(df_cust)\n",
        "\n",
        "                # Get the cluster labels for each data point\n",
        "                labels = cluster.labels_\n",
        "\n",
        "                score = 0\n",
        "                # Calculate silhouette score\n",
        "                if len(np.unique(labels.T)) > 1:\n",
        "                    score = silhouette_score(df, labels)\n",
        "                    sil_score.append(score)\n",
        "\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_params = (affinity, linkage, distance_threshold)\n",
        "\n",
        "                x_min = df_cust[:, x].min()\n",
        "                x_max = df_cust[:, x].max()\n",
        "\n",
        "                ax[k].scatter(df_cust[:, x], df_cust[:, y], c=labels, cmap=\"rainbow\")\n",
        "                ax[k].set_title(\n",
        "                    f\"Distance Threshold: {distance_threshold:.5f}\\nSilhouette Score: {score:.2f}\"\n",
        "                )\n",
        "\n",
        "                ax[k].set_xlabel(f\"{x_label}\")\n",
        "                ax[k].set_ylabel(f\"{y_label}\")\n",
        "\n",
        "                ax[k].set_xlim((x_min, x_max))\n",
        "\n",
        "                if np.any(labels.T):\n",
        "                    # get centroids\n",
        "                    clf = NearestCentroid()\n",
        "                    # print(HClusteringY)\n",
        "                    clf.fit(df_cust, labels)\n",
        "                    ax[k].scatter(\n",
        "                        clf.centroids_[:, 0],\n",
        "                        clf.centroids_[:, 1],\n",
        "                        c=\"black\",\n",
        "                        s=100,\n",
        "                        alpha=0.5,\n",
        "                        marker=\"x\",\n",
        "                    )\n",
        "\n",
        "            fig.suptitle(\n",
        "                f\"Agglomerative clustering with Affinity: {affinity} and Linkage: {linkage}\",y=1.01,fontsize=20)\n",
        "\n",
        "            fig.tight_layout()\n",
        "            plt.show()\n",
        "    return best_params, sil_score\n",
        "\n",
        "\n",
        "best_params, sil_score = aggro_hierarchal_cust(0.05,1.5,2,4,X)\n",
        "\n",
        "\n",
        "# create silhouette graph\n",
        "plt.figure(figsize=(15, 10))\n",
        "# plt.plot(range(0, 16), sil_score[0:16], \"bx-\", label=\"euclidean average\", color=\"r\")\n",
        "# plt.plot(range(16, 18), sil_score[16:18], \"bx-\", label=\"euclidean single\", color=\"b\")\n",
        "# plt.plot(range(18, 34), sil_score[18:34], \"bx-\", label=\"manhattan average\", color=\"g\")\n",
        "# plt.plot(range(34, 36), sil_score[34:36], \"bx-\", label=\"manhattan single\", color=\"m\")\n",
        "plt.plot(range(len(sil_score)) ,sil_score)\n",
        "plt.title(\"silhouette score\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "# plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best parameters: Affinity={best_params[0]}, Linkage={best_params[1]}, Distance Threshold={best_params[2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_df = pd.read_csv(\"Customer data.csv\")\n",
        "df = customer_df.drop(customer_df.columns[0], axis=1)\n",
        "scaler = prep.MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(df)\n",
        "X_dataframe = pd.DataFrame(normalized_data)\n",
        "X = pd.DataFrame.to_numpy(X_dataframe)\n",
        "\n",
        "neighb = NearestNeighbors(\n",
        "    n_neighbors=5\n",
        ")  # creating an object of the NearestNeighbors class\n",
        "nbrs = neighb.fit(X)  # fitting the data to the object\n",
        "distances, indices = nbrs.kneighbors(X)  # finding the nearest neighbours\n",
        "\n",
        "distances = np.sort(distances, axis=0)  # sorting the distances\n",
        "distances = distances[:, 1]  # taking the second column of the sorted distances\n",
        "plt.rcParams[\"figure.figsize\"] = (5, 3)  # setting the figure size\n",
        "plt.plot(distances)  # plotting the distances\n",
        "plt.show()  # showing the plot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DB SCAN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_df = pd.read_csv(\"Customer data.csv\")\n",
        "df = customer_df.drop(customer_df.columns[0], axis=1)\n",
        "scaler = prep.MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(df)\n",
        "X_dataframe = pd.DataFrame(normalized_data)\n",
        "X = pd.DataFrame.to_numpy(X_dataframe)\n",
        "\n",
        "\n",
        "def DBSCAN_cust(x, y, inv, cust_df):\n",
        "    eps_values = np.linspace(0.01,2, num=30)\n",
        "    if inv == False:\n",
        "        min_samples_values = np.linspace(5, 25, num=30)\n",
        "    else:\n",
        "        min_samples_values = np.linspace(25, 5, num=30)\n",
        "\n",
        "    fig, axs = plt.subplots(6, 5, figsize=(35, 35))\n",
        "\n",
        "    x_label = df.columns[x]\n",
        "    y_label = df.columns[y]\n",
        "\n",
        "    best_silhouette_score = -1\n",
        "    best_eps = None\n",
        "    best_min_samples = None\n",
        "    sil_scores = []\n",
        "\n",
        "    ax = axs.ravel()\n",
        "    for i in range(len(eps_values)):\n",
        "        eps = eps_values[i]\n",
        "        min_samples = min_samples_values[i]\n",
        "\n",
        "        db = DBSCAN(eps=eps, min_samples=min_samples).fit(cust_df)\n",
        "        labels = db.labels_\n",
        "\n",
        "        ax[i].scatter(cust_df[:, x], cust_df[:, y], c=labels, cmap=\"gist_rainbow\")\n",
        "        ax[i].set_title(f'eps={eps:.2f}, min_samples={min_samples:.2f}')\n",
        "        ax[i].set_xlabel(f\"{x_label}\")\n",
        "        ax[i].set_ylabel(f\"{y_label}\")\n",
        "\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "        if n_clusters > 1:\n",
        "            silhouette_avg = silhouette_score(cust_df, labels)\n",
        "            sil_scores.append(silhouette_avg)\n",
        "\n",
        "            if silhouette_avg > best_silhouette_score:\n",
        "                best_silhouette_score = silhouette_avg\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "\n",
        "    fig.suptitle(f\"DB SCAN of {x_label} versus {y_label}\", y=1.01)\n",
        "    fig.tight_layout()\n",
        "    return best_eps, best_min_samples, sil_scores\n",
        "\n",
        "\n",
        "# for i in range(1, len(customer_df.keys())):\n",
        "#     if i != 3:\n",
        "#         DBSCAN_cust(i, 3)\n",
        "\n",
        "# for i in range(1, len(customer_df.keys())):\n",
        "#     if i != 5:\n",
        "#         DBSCAN_cust(i, 5)\n",
        "\n",
        "best_eps, best_min_samples, sil_scores = DBSCAN_cust(2, 4, True, X)\n",
        "DBSCAN_cust(3, 4, False, X)\n",
        "DBSCAN_cust(6, 3, False, X)\n",
        "DBSCAN_cust(2, 5, True, X)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(len(sil_scores)), sil_scores)\n",
        "plt.title(\"silhouette score\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best eps: {best_eps:.2f}\")\n",
        "print(f\"Best min samples: {best_min_samples}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GMM clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_df = pd.read_csv(\"Customer data.csv\")\n",
        "df = customer_df.drop(customer_df.columns[0], axis=1)\n",
        "scaler = prep.MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(df)\n",
        "X_dataframe = pd.DataFrame(normalized_data)\n",
        "X = pd.DataFrame.to_numpy(X_dataframe)\n",
        "\n",
        "\n",
        "def GMM_cust(X):\n",
        "    n_components_range = range(1, 21)\n",
        "    covariance_types = [\"spherical\", \"tied\", \"diag\", \"full\"]\n",
        "\n",
        "    for index, covariance_type in enumerate(covariance_types):\n",
        "        plt.figure(figsize=(30, 18))\n",
        "        for n_components in n_components_range:\n",
        "            gmm = GaussianMixture(\n",
        "                n_components=n_components, covariance_type=covariance_type\n",
        "            )\n",
        "            gmm.fit(X)\n",
        "\n",
        "            plt.subplot(4, 5, n_components)\n",
        "\n",
        "            x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
        "            y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
        "\n",
        "\n",
        "            xx, yy = np.meshgrid(np.linspace(x_min, x_max), np.linspace(y_min, y_max))\n",
        "            # print(np.c_[xx.ravel(), yy.ravel()].shape)\n",
        "            Z = gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
        "\n",
        "            Z = Z.reshape(xx.shape)\n",
        "            plt.contour(xx, yy, Z)\n",
        "\n",
        "            plt.scatter(X[:, 0], X[:, 1], c=gmm.predict(X), cmap=\"gist_rainbow\")\n",
        "            plt.title(f\"n_components={n_components}\")\n",
        "\n",
        "        plt.suptitle(\n",
        "            f'GMM with {covariance_type} covariance for '\n",
        "        )\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "GMM_cust(X[:,2:5:2])\n",
        "# print(X[:,2:5:2])\n",
        "\n",
        "# GMM_cust(X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extra requirements\n",
        "- use PCA to reduce dimensionality\n",
        "- Test different distance function other then Euclidean and study their effects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plotnine import *\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.datasets import make_blobs\n",
        "import sklearn.preprocessing as prep\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# StandardScaler is a function to normalize the data\n",
        "# You may also check MinMaxScaler and MaxAbsScaler\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "# hierarchical clustering\n",
        "# for cdist in distortions graph\n",
        "\n",
        "%matplotlib inline\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_df = pd.read_csv(\"Customer data.csv\")\n",
        "df = customer_df.drop(customer_df.columns[0], axis=1)\n",
        "scaler = prep.MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(df)\n",
        "X_dataframe = pd.DataFrame(normalized_data)\n",
        "X = pd.DataFrame.to_numpy(X_dataframe)\n",
        "\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "reduced_X = pca.fit_transform(X)\n",
        "\n",
        "\n",
        "sil_scores_kmeans, distortions = kmeans_cust(0, 1, reduced_X)\n",
        "\n",
        "# create elbow graph\n",
        "plt.figure()\n",
        "plt.plot(range(len(distortions)), distortions)\n",
        "plt.title(\"The elbow method\")\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.ylabel(\"SUm of squares distance\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "# create and plot sil graph\n",
        "plt.figure()\n",
        "plt.plot(range(len(sil_scores_kmeans)), sil_scores_kmeans)\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Silhouette Score\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "best_params_aggro, sil_score_aggro = aggro_hierarchal_cust(0.05,1.5,0,1,reduced_X)\n",
        "\n",
        "# create silhouette graph\n",
        "plt.figure(figsize=(15, 10))\n",
        "# plt.plot(range(0, 16), sil_score[0:16], \"bx-\", label=\"euclidean average\", color=\"r\")\n",
        "# plt.plot(range(16, 18), sil_score[16:18], \"bx-\", label=\"euclidean single\", color=\"b\")\n",
        "# plt.plot(range(18, 34), sil_score[18:34], \"bx-\", label=\"manhattan average\", color=\"g\")\n",
        "# plt.plot(range(34, 36), sil_score[34:36], \"bx-\", label=\"manhattan single\", color=\"m\")\n",
        "plt.plot(range(len(sil_score_aggro)) ,sil_score_aggro)\n",
        "plt.title(\"silhouette score\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "# plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best parameters: Affinity={best_params_aggro[0]}, Linkage={best_params_aggro[1]}, Distance Threshold={best_params_aggro[2]}\")\n",
        "\n",
        "\n",
        "best_eps, best_min_samples, sil_scores = DBSCAN_cust(0, 1, True,reduced_X)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(len(sil_scores)), sil_scores)\n",
        "plt.title(\"silhouette score\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best eps: {best_eps:.2f}\")\n",
        "print(f\"Best min samples: {best_min_samples}\")\n",
        "\n",
        "GMM_cust(reduced_X)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Mahalanobis Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plotnine import *\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster import hierarchy\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.neighbors import NearestCentroid\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.datasets import make_blobs\n",
        "import sklearn.preprocessing as prep\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# StandardScaler is a function to normalize the data\n",
        "# You may also check MinMaxScaler and MaxAbsScaler\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "# hierarchical clustering\n",
        "# for cdist in distortions graph\n",
        "\n",
        "%matplotlib inline\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import mahalanobis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mahalanobis_distance(x, y):\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    VI = np.linalg.inv(np.cov(x.T))\n",
        "    return mahalanobis(x, y, VI)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### K means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "customer_df = pd.read_csv(\"Customer data.csv\")\n",
        "df = customer_df.drop(customer_df.columns[0], axis=1)\n",
        "scaler = prep.MinMaxScaler()\n",
        "normalized_data = scaler.fit_transform(df)\n",
        "X_dataframe = pd.DataFrame(normalized_data)\n",
        "X = pd.DataFrame.to_numpy(X_dataframe)\n",
        "\n",
        "\n",
        "def DBSCAN_cust(x, y, inv, cust_df):\n",
        "    eps_values = np.linspace(0.01,2, num=30)\n",
        "    if inv == False:\n",
        "        min_samples_values = np.linspace(5, 25, num=30)\n",
        "    else:\n",
        "        min_samples_values = np.linspace(25, 5, num=30)\n",
        "\n",
        "    fig, axs = plt.subplots(6, 5, figsize=(35, 35))\n",
        "\n",
        "    x_label = df.columns[x]\n",
        "    y_label = df.columns[y]\n",
        "\n",
        "    best_silhouette_score = -1\n",
        "    best_eps = None\n",
        "    best_min_samples = None\n",
        "    sil_scores = []\n",
        "\n",
        "    ax = axs.ravel()\n",
        "    for i in range(len(eps_values)):\n",
        "        eps = eps_values[i]\n",
        "        min_samples = min_samples_values[i]\n",
        "\n",
        "        # cov_X = X[:,x] \n",
        "\n",
        "        cov_X = np.stack((X[:,x],X[:,y]))\n",
        "\n",
        "        metric_params={'V':np.cov(cov_X)}\n",
        "\n",
        "        db = DBSCAN(eps=eps, min_samples=min_samples,metric=\"mahalanobis\",metric_params=metric_params).fit(cust_df)\n",
        "        labels = db.labels_\n",
        "\n",
        "        ax[i].scatter(cust_df[:, x], cust_df[:, y], c=labels, cmap=\"gist_rainbow\")\n",
        "        ax[i].set_title(f'eps={eps:.2f}, min_samples={min_samples:.2f}')\n",
        "        ax[i].set_xlabel(f\"{x_label}\")\n",
        "        ax[i].set_ylabel(f\"{y_label}\")\n",
        "\n",
        "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "        if n_clusters > 1:\n",
        "            silhouette_avg = silhouette_score(cust_df, labels)\n",
        "            sil_scores.append(silhouette_avg)\n",
        "\n",
        "            if silhouette_avg > best_silhouette_score:\n",
        "                best_silhouette_score = silhouette_avg\n",
        "                best_eps = eps\n",
        "                best_min_samples = min_samples\n",
        "\n",
        "    fig.suptitle(f\"DB SCAN of {x_label} versus {y_label}\", y=1.01)\n",
        "    fig.tight_layout()\n",
        "    return best_eps, best_min_samples, sil_scores\n",
        "\n",
        "\n",
        "# for i in range(1, len(customer_df.keys())):\n",
        "#     if i != 3:\n",
        "#         DBSCAN_cust(i, 3)\n",
        "\n",
        "# for i in range(1, len(customer_df.keys())):\n",
        "#     if i != 5:\n",
        "#         DBSCAN_cust(i, 5)\n",
        "\n",
        "best_eps, best_min_samples, sil_scores = DBSCAN_cust(2, 4, True, X)\n",
        "DBSCAN_cust(3, 4, False, X)\n",
        "DBSCAN_cust(6, 3, False, X)\n",
        "DBSCAN_cust(2, 5, True, X)\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(len(sil_scores)), sil_scores)\n",
        "plt.title(\"silhouette score\")\n",
        "plt.xlabel(\"different parameters\")\n",
        "plt.ylabel(\"score\")  # within cluster sum of squares\n",
        "plt.show()\n",
        "\n",
        "print(f\"Best eps: {best_eps:.2f}\")\n",
        "print(f\"Best min samples: {best_min_samples}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Clustering Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
